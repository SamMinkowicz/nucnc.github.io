---
layout: page
title: Events
---
Due to the pandemic, events are being held on Zoom. Join our Slack channel or sign up to our listserv for Zoom meeting details.<br>

---
<h2 align="center">Past Events </h2>

## Hidden brain states disentangle the roles of distinct oscillatory processes in creative problem solving
**Yuhua Yu, Northwestern University** <br>
**May 26th, 2022, 12:00 PM (Zoom)** <br>
Oscillatory activities have important roles in human cognitive functions and creative performance. Previous works have found various oscillations in theta and alpha bands, from distinct spatial locations, involved in different stages of a trial. In this project, we attempt to obtain a wholistic view of cognitive processes recruited during creative problem solving by applying an unsupervised machine learning algorithm to electrophysiological signals. EEG data were recorded when participants attempted Compound Remote Association verbal puzzles. Raw data underwent blind source separation and time-frequency transformation, and the resulting time series data were analyzed using a hidden Markov model. Multiple hidden states featuring activation in lower or upper alpha bands (as well as theta and beta) with distinct spatial distributions emerged from this data-driven approach. Importantly, the derived brain states exhibit differential profiles related to the trial stages (onset versus response) and outcomes (whether and how the problem was solved), suggesting their different cognitive functions. Moreover, a novel temporal connectivity analysis reveals specific transition patterns between the states. The results suggest a dynamic interplay of differentiable oscillatory processes during the course of creative problem solving.<br>

## Machine learning approaches to neuronal classification
**Zachary Frazier Jessen, Northwestern University** <br>
**March 24th, 2022, 12:00 PM (Zoom)** <br>
Neuronal classification is a critical tool to further our understanding of neural circuits and their computations, but efforts to classify neurons have been hindered by a combination of technical and conceptual hurdles. On the technical end, collection of data of sufficient quantity and complexity remains challenging. Conceptually, in the absence of a universal ground truth, most classification schemes are operative, and a balance has to be struck in the level of granularity that is achieved: a grouping can become either too broad or too specific in a given context. In this talk, I will discuss efforts to classify neurons using morphological, physiological, and molecular techniques, focusing on our recent work in unifying these modalities in the context of mouse retinal ganglion cells. I will highlight how semi-supervised and transfer learning can aid in the synthesis of scientific data sets into a coherent classification. I will also compare traditional classification approaches to those using machine learning and their respective limitations. Future advances in our understanding of neuronal typology are expected to come from leveraging our knowledge of their connectivity patterns through graph-based classification.<br>


## Self-supervised learning as a gateway to reveal underlying dynamics in animal behavior
**Kevin Luxem, Leibniz Institute for Neurobiology, Magdeburg** <br>
**February 18th, 2022, 12:00 PM (Zoom)** <br>
The brain is a dynamical system and its dynamics are reflected in the actions it performs. In order to understand brain functions, observable motion is a fundamental resource. Technological approaches towards understanding and dissecting the spatiotemporal order of animal motion are still in its infancy compared to approaches aiming at dissecting brain functions. Recent advances in representation and self-supervised learning can be leveraged to learn robust representations of animal behavior in an unprecedented manner. In my work, I explore combining autoregressive models with the notion of variational auto-encoding to learn robust spatiotemporal embeddings of animal behavior. Finally, self-supervised methods open the possibility to learn from neural and behavioral data jointly, which is a promising direction in the field of computational neuroethology.<br>


## Bioinformatic approaches to study adaptive immunity in neurodegenerative disease
**Dr. David M. Gate, Northwestern University** <br>
**November 18, 2021 12:00 PM (Zoom)**<br>
In this featured faculty talk, Dr. Gate will discuss the computational and bioinformatic techniques central to his recent publications in [Nature](https://www.nature.com/articles/s41586-019-1895-7), [Science](https://www.science.org/doi/full/10.1126/science.abf7266), and [Molecular Neurodegeneration](https://link.springer.com/article/10.1186/s13024-021-00423-w) identifying a role for T cells in neurodegenerative disease using multi-omics approaches. While activation of the innate immune system has long been linked to neurodegeneration, these results are among the first to establish a novel role for adaptive immunity. Dr. Gate is a recent hire in the Ken and Ruth Davee Department of Neurology from the Wyss-Coray laboratory at Stanford University.<br>


## Task-specific whole-cortical states during decision making
**Dr. Lucas Pinto, Northwestern University** <br>
**October 21, 2021 12:00 PM (Zoom)**<br>
A central question in the neuroscience of decision making is to what extent computations are distributed or localized in specific brain areas. In my talk, I will argue that the degree of localization of cortical function is not static, but rather depends crucially on the complexity of the underlying decision-making computations. I will first discuss experiments involving systematic optogenetic mapping and wide-field calcium imaging across the dorsal cortex. Specifically, I use these techniques to compare three related navigational decision tasks in virtual reality, happening in the same virtual maze but requiring different underlying computations. The results suggest that tasks with higher cognitive demands engage more distributed cortical circuits, with less correlated activity across different cortical regions. I will also discuss recent findings suggesting that, for a decision-making task requiring gradual accumulation of sensory evidence, widespread cortical areas contribute to evidence accrual on different timescales, exploiting an existing hierarchy of intrinsic temporal integration windows. Taken together, our results suggest that changes in the underlying computations result in a reorganization of whole-cortical dynamics, a concept which I refer to as task-specific cortical states. Understanding the properties and circuit mechanisms underlying our ability to switch between and maintain these states is the primary focus of my new lab here at Northwestern.<br>


## Dissecting how recurrent neural networks process noisy images
**Dr. Grace Lindsay, UCL** <br>
**September 2, 2021 12:00 PM (Zoom)**<br>
In the first half of this session, Dr. Lindsay will discuss her work on recurrent neural networks in visual perception.<br>

**Abstract:** Psychophysics studies have shown that visual perception of noisy or degraded images is enhanced when participants are given more time to process the image; this is believed to be due to the influence of recurrent connections in the visual system. I will share some recent work showing how convolutional neural networks with either bio-inspired or task-trained recurrent connections can replicate these behavioral findings, but do so with different neural mechanisms.<br>

In the second half of this session, we will discuss Dr. Lindsay's new book: [Models of the mind](https://www.bloomsbury.com/us/models-of-the-mind-9781472966452/){:target="_blank"}.<br>


## Sheaves for Neuroscientists
**Mark Agrios, NUIN** <br>
**May 6, 2021 12:00 PM (Zoom)** <br>
As experimental methods improve, we are now faced with the challenge of how to analyze data that can interact with itself at many different levels or organization. Here, we will introduce the often-intimidating mathematical structures called sheaves and discuss how they can be used to probe questions in biology and neuroscience.<br>
Please refer to this [write-up](http://markagrios.net/docs/intro_to_sheaves.pdf) for a brief introduction to sheaves.

## Weighted Correlation Network Analysis Reveals an Asd-specific Gene Module Expressed by Neurons in a Cerebral Organoid Model of Pacs1 Syndrome
**Lauren Rylaarsdam, NUIN** <br>
**May 20, 2021 12:00 PM (Zoom)**<br>
PACS1 syndrome is a neurodevelopmental disorder characterized by intellectual disability, autism spectrum disorder (ASD), epilepsy, distinct craniofacial abnormalities, and global developmental delays. PACS1 syndrome results from a single de novo missense p.R203W variant in phosphofurin acidic cluster sorting protein 1 (PACS1), which is involved in shuttling targets to the trans-Golgi Network. Almost no previous research has been done on how this variant in PACS1 affects the developing nervous system, and very few therapeutic options are available for patients. In order to begin to address the impact of the R203W variant on different cell types in the developing brain, I performed single-cell RNA sequencing on cerebral organoids generated from patient and control-derived stem cells. I found a total of 199 unique differentially expressed genes between PACS1 syndrome and control organoids. Differentially expressed genes - particularly in neurons - are enriched for OMIM risk genes of other neurodevelopmental disorders with overlapping phenotypes to PACS1 syndrome. Weighted gene co-expression network analysis (WGCNA) of the neuron group revealed four gene expression modules that were significantly correlated with genotype. One of these modules heavily overlaps with ASD-specific modules identified in other studies, suggesting that PACS1 syndrome likely shares convergent disease mechanisms with ASD. This work begins to provide a better understanding for how the PACS1 R203W variant affects the developing nervous system, opening further avenues of study to determine disease mechanisms and eventually develop therapies for patients.

## GED: A flexible family of versatile methods for hypothesis-driven multivariate decompositions
**Dr. Mike X Cohen, Radboud University** <br>
**April 1, 2021 12:00 PM (Zoom)** <br>
The goal of my presentation is to introduce a powerful yet under-utilized mathematical equation that is surprisingly effective at uncovering spatiotemporal patterns that are embedded in data -- but that might be inaccessible in traditional analysis methods due to low SNR or sparse spatial distribution. If you flunked calculus, then don't worry: the math is really easy, and I'll spend most of the time discussing intuition, simulations, and applications in real data. I will also spend some time in the beginning of the talk providing a bird's-eye-view of the empirical research in my lab, which focuses on mesoscale brain dynamics associated with error monitoring and response competition.

## The Neural Population Dynamics of Freely Behaving Mice
**Dr. Ann Kennedy, Northwestern University** <br>
**February 18, 2021 11:00 AM (Zoom)** <br>
In order to survive and reproduce, animals must produce a diverse range of innate and learned behaviors in a flexible and context-dependent manner. The computational task of forming an internal representation of an animal’s environment and translating that to the selection of goal-directed actions is dependent on the coordinated activity of multiple brain areas. The Kennedy lab works with experimentalists to dissect the neural correlates of behavior in multiple interconnected hypothalamic nuclei of freely behaving mice. In this talk, I will present our recent analyses of neural population tuning and dynamics, and discuss what we think these analyses are telling us about the interaction between different neural populations during behavior.

## Book Club Discussion
**Moderator: Maggie Swerdloff** <br>
**December 10, 2020 12:30 PM (Zoom)**

Ignorance: How it Drives Science - Stuart Firestein

## Journal Club Discussion
**Moderators: Maite Azcorra and Vivek Sagar** <br>
**December 3, 2020 12:30 PM (Zoom)**
1. On the Integration of Space, Time, and Memory – Eichenbaum, Neuron 2017 [https://www.sciencedirect.com/science/article/pii/S0896627317305603](https://www.sciencedirect.com/science/article/pii/S0896627317305603)

2. High-amplitude co-fluctuations in cortical activity drive resting-state functional connectivity – PNAS 2020 [https://www.pnas.org/content/early/2020/10/21/2005531117](https://www.pnas.org/content/early/2020/10/21/2005531117)

## Machine Learning Basics: How do Computers See?
**Moderator: Nathan Whitmore** <br>
**November 12, 2020 12:30 PM (Zoom)** <br>
Over the last decade, computer image recognition has risen to rival human vision in applications from from unlocking your phone to detecting tumors in CT scans. The key to this success has been convolutional neural networks (CNNs), which are simple yet incredibly powerful machine learning systems. This meeting will focus on developing an intuitive understanding of how these networks work, and how you can use them to detect patterns in both image and non-image data. <br>
Optional reading: [https://towardsdatascience.com/simple-introduction-to-convolutional-neural-networks-cdf8d3077bac](https://towardsdatascience.com/simple-introduction-to-convolutional-neural-networks-cdf8d3077bac)

## MATLAB Programming Practices
**Vivek Sagar** <br>
**September 28, 2020 5 PM** <br>
MATLAB is a high-level matrix/array language with an intuitive IDE and an interactive graphics system that makes it an easy language for the beginners. MATLAB codes however, are susceptible to sub-optimal performance if good programming practices are not followed. In this module we will go over some techniques and tools to optimize the code performance in MATLAB. In this hands on session, we will cover: MATLAB data structures, working with MATLAB Profiler, important practices such as preallocation and vectorization, fast referencing operations, function handles and memory optimization. This is part 1 of a 2 part module. Second module will feature parallel processing, GPU acceleration and working with Mex scripts.

## Alveolitis In Severe Sars-CoV-2 Pneumonia Is Driven by Self-sustaining Circuits Between Infected Alveolar Macrophages and T Cells
**Rogan Grant and Nikolay Markov** <br>
**September 7, 2020 5 PM** <br>
Some patients infected with Severe Acute Respiratory Syndrome Coronavirus-2 (SARS-CoV-2) develop severe pneumonia and the acute respiratory distress syndrome (ARDS). Distinct clinical features in these patients have led to speculation that the immune response to virus in the SARS-CoV-2-infected alveolus differs from other types of pneumonia. We collected bronchoalveolar lavage fluid samples from 86 patients with SARS-CoV-2-induced respiratory failure and 252 patients with known or suspected pneumonia from other pathogens and subjected them to flow cytometry and bulk transcriptomic profiling. We performed single-cell RNA-Seq in 5 bronchoalveolar lavage fluid samples collected from patients with severe COVID-19 within 48 hours of intubation. In the majority of patients with SARS-CoV-2 infection at the onset of mechanical ventilation, the alveolar space is persistently enriched in CD4+ and CD8+ T cells, without neutrophilia. Bulk and single cell transcriptomic profiling suggest SARS-CoV-2 infects alveolar macrophages that respond by recruiting T cells. These T cells release interferon-gamma to induce inflammatory cytokine release from alveolar macrophages and further promote T cell recruitment. Our results suggest SARS-CoV-2 causes a slowly unfolding, spatially-limited alveolitis in which alveolar macrophages harboring SARS-CoV-2 transcripts and T cells form a positive feedback loop that drives progressive alveolar inflammation. <br>
This talk is accompanied by an online resource: https://www.nupulmonary.org/covid-19/

## Book club: The deep learning book, Ian Goodfellow
**July 27, 2020 5 PM**<br>
This is the second installment of the book club. We will discuss sections from Ian Goodfellow's deep learning book. This discussion will be focused on chapter 5 of the book, covering the basics of machine learning theory. Book is available for free at the following link: https://www.deeplearningbook.org/contents/ml.html

## Alveolitis In Severe Sars-CoV-2 Pneumonia Is Driven by Self-sustaining Circuits Between Infected Alveolar Macrophages and T Cells
**Torben Noto** <br>
**July 13, 2020 5 PM** <br>
Emotion and breathing are intimately linked. We laugh, cry, gasp, experience shortness-of-breath and modify our respiration in other characteristic patterns directly based on emotion. In each of these 4 behaviors, emotion disrupts breathing, a vital process that we must constantly perform to survive. Considerable progress has been made investigating the circuits that generate natural breathing rhythms; however, the circuits that integrate emotion into respiratory behaviors are unknown. Recent work in epilepsy patients has shown that direct electrical stimulation of the amygdala disrupts nasal breathing, typically causing apnea. Here, we quantify the psychometrics of this circuit by measuring how quickly breathing is modulated in response to human faces, a well-established amygdala stimulus. We hypothesize that time delay at which breathing is reduced following the presentation of an amygdala stimulus will be consistent with that of a subcortical amygdala circuit. Our research represents a possible biomarker for anxiety and could indicate maladaptive associations between emotional and respiratory networks. Finally, in a single iEEG patient we find that theta power in local field potentials of the amygdala strongly correlate with face-induced respiratory reductions. Together, these findings support a possible role for the amygdala in acute respiratory reductions and indicate future avenues to directly test the functional properties of this circuit.

## Book club: The deep learning book, Ian Goodfellow, Chapter 5
**June 15, 2020 5 PM**<br>
We will discuss sections from Ian Goodfellow's deep learning book. This discussion will be focused on the first 30 pages of chapter 5 of the book, covering the basics of machine learning theory. Book is available for free at the following link: https://www.deeplearningbook.org/contents/ml.html

## De Novo Genome Assembly: Tackling Structural Variation
**Alex Telenson** <br>
**June 1, 2020 5 PM**<br>
Assembling genomes from billions of short read segments has proven difficult, but manageable. Traditional assembly relies on a species reference genome to use as the scaffold or foundation upon which SNPs, CNV and short indels are easily identified. However, phenotypes caused by large structural variation in the genome has proven much more difficult to ascertain. I will elaborate these challenges and talk about potential paths forward.

## Decoding Sleep with Neural Networks
**Nathan Whitmore** <br>
**May 11, 2020 5 PM**<br>
Join us for a discussion on the interface of deep learning and sleep. Nathan will talk about his lab's work to decode sleep stages using deep neural networks and smartwatches, and how it fits into their overall goal of developing devices that can manipulate sleep and memory.

## Book Club Discussion: "Symmetry" by Hermann Weyl
**April 6, 2020 6 PM** <br>
Penned by one of the most influential mathematicians of the 20th century, Symmetry gradually exposes the reader to abstract mathematical notion of symmetry with the help of intuitive real-life examples.

## Revisiting Linear Algebra
**Moderator: Vivek Sagar** <br>
**January 27, 2020 5 PM (Wieboldt 421, Telec. to Tech L251)** <br>
Has reading up on eigenvalue problems been on your to-do list for over a year? Do you simply pretend that you understand things when your coworker is talking about matrix diagonalization or singular value decomposition in the lab meeting? Do you now realize that PCA is cool but wish you had paid more attention in the linear algebra class in college? Or perhaps you are just looking for a safe, non-judgmental space to let your inner mathematician out. In any case, join us for the first NUCNC event of this decade – “Understanding eigenvalue problems”. I will remind you of what eigenvalue problems are, where they can be used and how to solve them without a computer. We will also discuss how engineers use linear algebra to solve complicated problems of the universe, physicists use it to visualize the laws of the universe and mathematicians use it to create a universe of their own. <br>
Pre-requisites: Just know how to multiply matrices and what a matrix determinant and inverse is.

# November 4, 2019 5 PM (Tech L251, Telec. to Wieboldt 421)
**Featured Faculty Talk: Human Brain Networks and Hubs, Dr. Caterina Gratton**<br>
The human brain is organized into large-scale networks, or systems, of interacting brain regions. These interactions can be measured in humans with functional Magnetic Resonance Imaging (fMRI), by measuring correlations in the patterns of activity between different regions. Increasingly sophisticated techniques enable the mapping of brain networks at unprecedented levels of detail, but many questions still remain. In this presentation, I will tackle two recent studies that we have undertaken to better understand human brain networks and their contributions to brain function. In the first study, we examine whether the topology of brain networks – specifically, the presence of hub regions – is important for brain function, by examining the consequences of damage to these regions. In the second study, we examine the variability in brain networks within and across subjects at different time-scales. Jointly, these studies suggest that network topology has important implications for human brain function, and that measures of network organization are stable features that can be used to measure trait-like variability in brain organization. I will close with a pointer to a tutorial demonstrating four useful visualizations of functional networks.<br>

# September 9, 2019 5 PM (Pancoe 4113, Telec. to Wieboldt 421)
**The Shift from Life in Water to Life on Land Advantaged Planning in Visually-Guided Behavior, Ugurcan Mugan**<br>
Studies of animal decision making reveal two distinct, competing control paradigms: habit- and plan-based action selection. The habit system has largely been associated with the lateral striatum and its dopaminergic afferents. Conserved basal ganglia structure from lamprey to mammals suggests that habitual control evolved early and persisted through vertebrate evolution. On the other hand, the planning system requires the interaction between the hippocampus and the prefrontal cortex or its homolog in birds. While the hippocampus (and its homologs) exists in both aquatic and land vertebrates, there seems to be no known homolog of the PFC in non-mammalian aquatic vertebrates.<br>
Our prior research into the water-to-land-transition indicates large increases in both visual range and observed environmental complexity. We hypothesize that these changes advantaged neural structures promoting planning over habit. To test this hypothesis, we developed two simulations of predator-prey dynamics. First, we simulated aquatic conditions in which the prey’s visual range was varied in a simple environment. Second, we simulated terrestrial conditions in which we varied environmental complexity by adding clutter and extended the prey visual range to the whole environment, except as blocked by occlusions. In both simulations, the prey was configured to have either habit-based action selection, or plan-based action selection with a preset number of states it could forward simulate. Our aquatic simulations strongly suggest that planning, while advantaged in proportion to visual range, cannot improve performance over habit in simple environments. In line with this idea, the results of our terrestrial simulations indicate that in spatially simple environments (near-open and highly cluttered) planning is not advantaged over habit. In contrast, we find that in spatially complex environments at midrange clutter level, planning produces complex predator avoidance behaviors and significantly increases performance over habit. Moreover, these results suggest that forward shifts in neural representations and the modulation of theta power in the mPFC during spatial navigation may be related to the distribution of cell connectedness. Notably, high cost choice points occur in environments that have adjacent regions of highly and poorly connected cells where planning becomes crucial. Interestingly, we find that complex environments resemble terrestrial habitats, supporting our hypothesis that a habitat shift advantaged planning over habit and is therefore likely to have been a key factor in the evolution of planning circuitry in select terrestrial vertebrates.<br>

# July 1, 2019, 5 PM
**Neuronal multi-omics, Dr. Yue Yang**<br>
Neuronal-activity-dependent transcription couples sensory experience to adaptive responses of the brain including learning and memory. I will discuss how sensory experience remodels chromatin architecture in the adult brain in vivo to orchestrate activity-dependent transcription and learning and memory. We use large-scale chromatin profiling, CRISPR genetics, and systems neuroscience approaches to study the role of long-distance enhancer-promoter and compartment interactions in the control of neural circuits in the brain.

# May 20, 2019, 5PM
**Applications of Information Theory in Early Sensory Systems, Dr. Greg Schwartz**<br>
Dr. Schwartz will talk about the basics of information theory from the first principles, including the history of the field. He will then provide some examples of the use of information theory in neuroscience. He will later discuss selected portions of the attached paper.

# May 13, 2019, 4PM
**Decoding Neural Oscillations, Nathan Whitmore**<br>
We will talk about how the Computational Neuroscience Club built a mind-controlled video game and how computational analysis can help us understand the brain's electrical signals.

# May 6, 2019, 5PM
**Introduction to Multivoxel Pattern Analysis (MVPA), Dr. Thorsten Kahnt**<br>
This session will provide a brief conceptual introduction to multivoxel pattern analysis (MVPA) for fMRI and discuss why, depending on the research question, using these methods can be advantageous. The focus will be on classification and regression approaches rather than representational similarity. We will discuss the general approach and step through one example. We will conclude by discussing potential mechanisms underlying multivoxel patterns and caveats of the method.

# April 22, 2019, 5 PM
**White Matter Tractography in DTI, Shiloh Cooper**<br>
Diffusion MRI is sensitive to water movements through tissues, and is used to characterize tissue structure based on the restriction of water movement. One example is white matter tractography, where the water pathways are assumed to travel in the same directions as bundles of axons traveling between cortical regions of interest. How can we create an accurate tractography algorithm (i.e., one that does not "jump" from one white matter tract to another when they cross within a voxel) without using anatomical priors and biasing the algorithm?

# April 8, 2019, 5 PM
**Single-Cell RNA Sequencing: A neuro-centric introduction, Rogan Grant**<br>
In the last decade, high-throughput sequencing technologies have enabled researchers to dissect transcriptional responses to a wide array of stimuli. Unfortunately, “bulk” tissue approaches have often been of limited utility to the neuroscience community, owing to the vast cellular complexity of the mammalian brain. The advent of single-cell RNA sequencing (scRNA-seq) therefore has the potential to enable researchers to study transcriptional remodeling within the brain across a wide variety of experimental manipulations. In this interactive lecture, we will process a publicly available single scRNA-seq dataset from raw sequencing data to the assignment and exploration of cell-type clusters. Particular emphasis will be given to the strengths and weaknesses of the technique, as well as current methods for data processing, visualization, and differential expression analysis. Attendees are strongly encouraged to bring a computer to the lecture.

# March 11, 2019, 5 PM
**Big data and data science in neuroscience, Torben Noto**<br>
"Big Data" and "Data science" are concepts that have gained considerable attention over the last few years but have unclear definitions and are not traditionally taught in academic circles. In the first part of this talk I will first introduce some basic ideas in data science in a way that will hopefully separate its rhetoric from its value and potentially think about analyzing your data in new ways. Data science is becoming especially important as publicly available neuroscience datasets like the Allen Brain Atlas rapidly emerge. These datasets provide many exciting opportunities in research. However, using these datasets can be tricky. In the second part of this talk we'll discuss some of these datasets as well as the basics of using them. In the final part of the talk, we'll use what we've learned to discuss the [methods paper behind Neurosynth](https://www.nature.com/articles/nmeth.1635), a metascientific fMRI mapping tool.

# February 25, 2019 5PM
**Recurrent Neural Networks for Continuous Neural Decoding, Maite Azcorra Sedano**<br>
In this session we will discuss the challenges of predicting continuous variables from neural activity. We will introduce different methods to approach this problem and focus on LSTM (Long-Short Term Memory) recurrent neural networks and their benefits. We will also discuss important things to keep in mind when implementing such networks, such as cross validation, data preprocessing, optimization of hyperparameters etc. We will finally over some real life examples of how to (and not to) implement LSTMs to predict mouse behavior from GCaMP neuronal activity.<br>
Prerequisites: Interest in Machine Learning!<br>

# February 11, 2019, 5PM
**Neural encoding and decoding in MATLAB, Vivek Sagar** <br>
For the first half of the discussion, we will briefly discuss principles of neural encoding and decoding based on the following [paper](https://www.sciencedirect.com/science/article/pii/S0079612306650310). Second half of the discussion will be based on practically implementing these models in MATLAB. We will construct an encoder (and decoder) for mapping spatial position to neural activity (and vice-versa) in MECIII grid cells in freely behaving rats. For this we will use the following [dataset](https://archive.norstore.no/pages/public/datasetDetail.jsf?id=8F6BE356-3277-475C-87B1-C7A977632DA7) and [codes](https://github.com/viveksgr/NUCNC_demo/tree/master/Scripts) for analysis.
Prerequisites: basic probability theory, coding in MATLAB
